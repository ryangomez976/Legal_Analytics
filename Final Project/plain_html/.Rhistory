2. Let's load the necessary packages.
+ If you see an error: "Erorr in library(libraryName) : there is no package called 'libraryName', run the command: install.packages("libraryName")
```{r load_packages, results="hide"}
library(tm)
library(RCurl)
library(XML)
library(SnowballC)
library(plyr)
library(class)
```
3. Import R code from GitHub that gives us some flexibility in converting html text to plain text (Thanks to Tony Breyal)
```{r import_}
url <- "https://raw.githubusercontent.com/tonybreyal/Blog-Reference-Functions/master/R/htmlToText/htmlToText.R"
script <- getURL(url, ssl.verifypeer=FALSE, useragent="R")
eval(parse(text = script),envir=.GlobalEnv)
```
4. Import a text collection of text files
```{r build_text_collection}
dirPath <- "~/Desktop/OneDrive/MSU Law/3L/Spring 2015/Legal Analytics/Text Mining/doc_sample/"
setwd(dirPath) #Set your working directory to the folder with all of your text files
`{r clean_corpus_text}
corpus <- Corpus(DirSource(dirPath), readerControl = list(language = "english"))
corpus <- tm_map(corpus, content_transformer(tolower)) #all lower case text
corpus <- tm_map(corpus, content_transformer(removePunctuation)) #remove punctuation
corpus <- tm_map(corpus, content_transformer(removeNumbers)) #remove numbers
corpus <- tm_map(corpus, content_transformer(stripWhitespace)) #strip white space
stopwords('english') # print stopwords that we will take out from the corpus text
corpus_clean <- tm_map(corpus, removeWords, stopwords("english")) # remove stopwords listed above
corpus <- corpus_clean # create a copy of the corpus for editing, and retain a master copy for later
corpus <- Corpus(DirSource(dirPath), readerControl = list(language = "english"))
corpus <- tm_map(corpus, content_transformer(tolower)) #all lower case text
corpus <- tm_map(corpus, content_transformer(removePunctuation)) #remove punctuation
corpus <- tm_map(corpus, content_transformer(removeNumbers)) #remove numbers
corpus <- tm_map(corpus, content_transformer(stripWhitespace)) #strip white space
stopwords('english') # print stopwords that we will take out from the corpus text
corpus_clean <- tm_map(corpus, removeWords, stopwords("english")) # remove stopwords listed above
corpus <- corpus_clean
corpus <- tm_map(corpus, stemDocument, language = "english")
tdm <- TermDocumentMatrix(corpus, control = list(minWordLength = 3))
tdm # print the summary of the term document matrix
tdm <- removeSparseTerms(tdm, 0.40)
tdm # print the summary again, after removing sparse terms. Notice that sparsity is much lower, with much better maximal term length
findFreqTerms(tdm, lowfreq=1000)
```{r sparse_terms}
tdm <- removeSparseTerms(tdm, 0.20)
findFreqTerms(tdm, lowfreq=1000)
tdm
findAssocs(x = tdm, term = "act", corlimit = 0.5)
library(wordcloud)
matrix <- as.matrix(tdm)
wordFreq.sort <- sort(rowSums(matrix), decreasing=T)
grayLevels <- gray( ( wordFreq.sort + 10 ) / (max(wordFreq.sort)+10))
library(RWeka)
```
```{r tokenize_things}
options(mc.cores=1) # This is a necessary line for the RWeka tokenizer to work on OS X
corpus <- corpus_clean # create a copy of the corpus for editing, and retain a master copy for later
corpus <- tm_map(corpus, stemDocument, language = "english") #stem the corpus
# Set up 3 tokenizers
gram1Tokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 1, max = 1))}
gram2Tokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2))}
gram3Tokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3))}
# Create 3 document matricies using different tokenizers
tdm1 <- TermDocumentMatrix(corpus, control=list(tokenize=gram1Tokenizer, minWordLength=3))
tdm2 <- TermDocumentMatrix(corpus, control = list(tokenize=gram2Tokenizer, minWordLength=3))
tdm3 <- TermDocumentMatrix(corpus, control = list(tokenize=gram3Tokenizer, minWordLength=3))
# Examine which tokenizer creates the best term document matrix
tdm1
tdm2
tdm3
tdm1 <- removeSparseTerms(tdm1, 0.40)
# print the summary again, after removing sparse terms. Notice that sparsity is much lower, with much better maximal term length
tdm1
View(matrix)
View(matrix)
df <- as.data.frame(inspect(tdm1))
View(df)
dft <- t(df) # transpose the data
View(df)
View(df)
View(dft)
dft <- data.frame(dft)
View(dft)
View(dft)
purchaseAgreements <- c("1853_0001077048-04-000086_1-amc_ex2-1.htm", "1853_0001077048-04-000400_1-ex10-1.htm", "2135_0000950134-02-007115_1-d97664exv10w1.txt", "2135_0000950134-05-005365_1-d23500exv2w1.htm", "2135_0000950134-05-011122_1-d25941exv2w1.htm", "2135_0000950134-05-021744_1-d30499exv2w1.htm", "1096509_0000950144-05-007529_1-g96346exv99wxay.htm", "1017137_0000950134-00-002505_1-2", "1026506_0001012870-00-001091_1-2")
class <- c()
class <- c(class,(ifelse((rownames(dft) %in% purchaseAgreements), 1, 0)))
View(dft)
View(dft)
class <- c(class,(ifelse((rownames(dft) %in% purchaseAgreements), 1, 0)))
class
dft <- cbind(dft, class)
View(dft)
dft$class <- as.factor(dft$class)
df <- as.data.frame(inspect(tdm1))
dft <- t(df) # transpose the data
dft <- data.frame(dft)
purchaseAgreements <- c("1853_0001077048-04-000086_1-amc_ex2-1.htm", "1853_0001077048-04-000400_1-ex10-1.htm", "2135_0000950134-02-007115_1-d97664exv10w1.txt", "2135_0000950134-05-005365_1-d23500exv2w1.htm", "2135_0000950134-05-011122_1-d25941exv2w1.htm", "2135_0000950134-05-021744_1-d30499exv2w1.htm", "1096509_0000950144-05-007529_1-g96346exv99wxay.htm", "1017137_0000950134-00-002505_1-2", "1026506_0001012870-00-001091_1-2")
class <- c()
class <- c(class,(ifelse((rownames(dft) %in% purchaseAgreements), 1, 0)))
dft <- cbind(dft, class, check.names = TRUE)
View(dft)
dft <- cbind(dft, class, class)
View(dft)
dft$class <- as.factor(dft$class)
```{r training_set}
training <- sample(nrow(dft), ceiling(nrow(dft) * 0.7))
test = (1:nrow(dft))[-training]
training <- dft[training,]
test <- dft[test,]
library(rpart)
patree <- rpart(class ~., data=training, method = "class")
patree.predictions.eval <- predict(patree,test,type="class")
confusionmatrix <- table(test$class,patree.predictions.eval)
confusionmatrix
tn <- confusionmatrix[1,1] #true negative
fp <- confusionmatrix[1,2] #false positive
fn <- confusionmatrix[2,1] #false negative
tp <- confusionmatrix[2,2]
accuracy.rate <- (tp+tn) / (tp + tn + fp + fn)
accuracy.rate
dftt <- transpose(dft)
dftt <- t(dft)
View(dftt)
dirPath <- "~/Desktop/Legal Analytics Final/plain_html/"
url <- "https://raw.githubusercontent.com/tonybreyal/Blog-Reference-Functions/master/R/htmlToText/htmlToText.R"
script <- getURL(url, ssl.verifypeer=FALSE, useragent="R")
eval(parse(text = script),envir=.GlobalEnv)
html2txt <- lapply(dirPath, htmlToText)
files <- list.files(dirPath)
html2txt <- lapply(files, htmlToText)
dirPath <- list.files(pattern="\\.(htm|html)$")
html2txt <- lapply(dirPath, htmlToText)
dirpath
dirPath
getwd()
setwd("~/Desktop/Legal Analytics Final/plain_html/")
dirPath <- list.files(pattern="\\.(htm|html)$")
html2txt <- lapply(dirPath, htmlToText)
html2txtclean <- sapply(html2txt, function(x) iconv(x, "latin1", "ASCII", sub=""))
corpus <- Corpus(VectorSource(html2txtclean))
tdm <- TermDocumentMatrix(corpus, control = list(minWordLength = 3))
# # tdm
tdm <- removeSparseTerms(tdm, 0.40)
tdm
matrix <- as.matrix(tdm)
View(matrix)
